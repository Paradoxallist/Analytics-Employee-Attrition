# ğŸ“Š Employee Attrition Prediction

Predict employee attrition using structured HR data with classical machine learning models and feature engineering.

---

## ğŸš€ Overview

This project solves a binary classification problem on a real HR dataset (from IBM) to predict whether an employee will leave the company.

Key ML steps:
- Exploratory Data Analysis (EDA)
- Feature Engineering
- Preprocessing Pipelines
- Model Selection and Evaluation

---

## ğŸ—‚ Project Structure

```
â”œâ”€â”€ config.py                     # Model parameter configuration
â”œâ”€â”€ main.py                       # Entry point: compares data versions and models
â”œâ”€â”€ preprocessor.py               # Cleans, transforms, and splits data
â”œâ”€â”€ feature_generator.py          # Generates arithmetic, logical, binned features
â”œâ”€â”€ visualizer.py                 # Correlation & importance visualizations
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ random_forest.py          # Random Forest wrapper
â”‚   â”œâ”€â”€ logistic_regression.py    # Logistic Regression with scaling
â”‚   â””â”€â”€ gradient_boosting.py      # Gradient Boosting model
â”œâ”€â”€ images/                          # Output plots for README
â”œâ”€â”€ WA_Fn-UseC_-HR-Employee-Attrition.csv  # Dataset
â””â”€â”€ README.md
```

---

## ğŸ“ˆ Pipeline Summary

### ğŸ”¹ 1. Data Cleaning
- Removed constant/irrelevant columns
- Target label (`Attrition`) encoded to `AttritionFlag`

### ğŸ”¹ 2. Correlation Analysis
- Visualized pairwise correlations
- Identified highly correlated groups

### ğŸ”¹ 3. Feature Versioning
Created multiple dataset versions for experimentation:

| Version      | Description                                      |
|--------------|--------------------------------------------------|
| `raw`        | Original dataset                                 |
| `cleaned`    | Dropped constant/irrelevant features             |
| `combined`   | Combined highly correlated features              |
| `filtered`   | Equivalent to combined (reserved for future)     |
| `gen_full`   | Auto-generated arithmetic/logical/binned features|
| `gen_pruned` | Correlation-pruned version (threshold = 0.8)     |

### ğŸ”¹ 4. Models Used
- âœ… `RandomForestClassifier` (base + tuned variants)
- âœ… `LogisticRegression` (`lbfgs`, scaled)
- âœ… `GradientBoostingClassifier`

### ğŸ”¹ 5. Evaluation Metric
- **ROC AUC Score** chosen due to class imbalance

---

## ğŸ“Š Visual Results

### ğŸ” Correlation Heatmap
![Correlation](images/Figure_5.png)

### ğŸ” Top 10 Highly Correlated Features
![Top 10 Corr](images/Figure_6.png)

### ğŸ“Š Model RandomForest Performance by Data Version 
![ROC AUC by Version](images/Figure_9.png)

### ğŸ“Š Final Model Comparison
![Final Model Comparison](images/Figure_10.png)

---

## ğŸ Results

| Version + Model             | ROC AUC Score |
|-----------------------------|---------------|
| `cleaned + GradientBoosting`     | **0.7966**   |
| `raw + GradientBoosting`         | 0.7903        |
| `raw + LogisticRegression`       | 0.7902        |
| `cleaned + LogisticRegression`   | 0.7892        |
| `combined + LogisticRegression`  | 0.7815        |
| `filtered + LogisticRegression`  | 0.7815        |

- GradientBoosting and LogisticRegression show the best results
- `cleaned` and `raw` versions are the most stable





## ğŸ“ Dataset Source
- [IBM HR Analytics Employee Attrition & Performance (Kaggle)](https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset)

